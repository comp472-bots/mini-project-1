{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6a01151",
   "metadata": {},
   "source": [
    "# Task 1: Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33334d69",
   "metadata": {},
   "source": [
    "#### 1. Download the BBC dataset provided on Moodle. The dataset, created by Greene and Cunningham, 2006 is a collection of 2225 documents from the BBC news website already categorized into 5 classes: business, entertainment, politics, sport, and tech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88a2d837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['business', 'entertainment', 'politics', 'README.TXT', 'sport', 'tech']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# The dataset can be found in the /data/BBC folder\n",
    "print(os.listdir(\"../data/BBC\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42442b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of instances in each class {'business': 510, 'entertainment': 386, 'politics': 417, 'sport': 511, 'tech': 401}\n"
     ]
    }
   ],
   "source": [
    "# Dictionary to hold the number of instances of each class\n",
    "# business/entertainment/politics/sport/tech\n",
    "category_dict = dict()\n",
    "\n",
    "category_dict[\"business\"] = len(os.listdir(\"../data/BBC/business\"))\n",
    "category_dict[\"entertainment\"] = len(os.listdir(\"../data/BBC/entertainment\"))\n",
    "category_dict[\"politics\"] = len(os.listdir(\"../data/BBC/politics\"))\n",
    "category_dict[\"sport\"] = len(os.listdir(\"../data/BBC/sport\"))\n",
    "category_dict[\"tech\"] = len(os.listdir(\"../data/BBC/tech\"))\n",
    "\n",
    "print(\"The number of instances in each class\", category_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30f8ae63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Texts: 2225\n"
     ]
    }
   ],
   "source": [
    "num_texts = 0\n",
    "\n",
    "for value in category_dict.values():\n",
    "    num_texts += value\n",
    "print('Total Number of Texts: {num_texts}'.format(num_texts=num_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f484909",
   "metadata": {},
   "source": [
    "#### 2. Plot the distribution of the instances in each class and save the graphic in a file called BBC-distribution.pdf. You may want to use matplotlib.pyplot and savefig to do this. This pre-analysis of the data set will allow you to determine if the classes are balanced, and which metric is more appropriate to use to evaluate the performance of your classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47293d43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmQAAAFNCAYAAACuWnPfAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAl00lEQVR4nO3deZglZX238fvLIqggiAwEgWEQMQomkjhBCSZqIIpGgRhRjMuoJIQEgxr1FSIajBJxI9GoiaiEEVQCioK4gaMsLshilDUoso4gm7JKUOD3/lFPy6Hp5cxyunqm7891nevUeU4tv6rq6v72U3VOpaqQJElSf9bouwBJkqS5zkAmSZLUMwOZJElSzwxkkiRJPTOQSZIk9cxAJkmS1DMDmTTDkvxnkreupHnNT3JHkjXb69OS/NXKmHeb31eSLFpZ81uG5b4zyU1JfjbTy56t+toXk0nyyiTfWonzOyTJMStrftKqxkAmrURJrkxyV5Lbk9yS5DtJ9kvym2OtqvarqncMOa9dpxqnqq6uqvWq6t6VUPuD/iBW1XOqavGKznsZ69gSeAOwXVX91gTvPyPJ0pWwnJUaXkdtRfbFwM/lHQOPD63sGoeo4y+TnNuWf10LmU+b6Tqk2WitvguQVkPPr6qvJ9kAeDrwAeApwKtW5kKSrFVV96zMec4SWwE3V9UNfReymnl+VX29r4Un+QfgQGA/4GvAr4DdgD2AldbTJq2q7CGTRqSqbq2qk4AXA4uSPBEgyVFJ3tmGN05ycutN+3mSM5OskeRoYD7wxdab8P+SLEhSSfZJcjXwjYG2wX+utklydpJbk5yYZKO2rAf1LI31wiXZDfhH4MVteT9s7/+mF6nVdXCSq5LckOSTLXQyUMeiJFe3041vmWzbJNmgTX9jm9/Bbf67AqcCj251HDXddm41viPJt1vP5ClJNm7vrZvkmCQ3t218TpJNkxwK/BHwocHeoiQfSHJNktuSnJfkjwaWc0iS41rdtye5KMnCgfe3THJCW6ebB3ugkrw6ySVJfpHka0m2au1J8q9te96a5Pyxn5NJ1nNsX7wyybeSvK/N84okz5luW00y322SfKPVfFOSTyXZcJj1au9PW0P7OflnYP+qOqGq7qyqX1fVF6vqTZNMc3ySn7XtckaS7Qfee26Si9t++GmSN7b2CY+n5dku0kzzB1Uasao6G1hKFwDGe0N7bx6wKV0oqqp6OXA1Xa/GelX1noFpng48AXj2JIt8BfBq4NHAPcAHh6jxq8C/AP/dlvekCUZ7ZXs8E3gMsB4w/rTX04DfBnYB3pbkCZMs8t+BDdp8nt5qflXrwXkOcG2r45XT1d78JV0P5CbAQ4A3tvZFbTlbAo+i6525q6reApwJvKYt5zVt/HOAHYCNgE8DxydZd2A5uwPHAhsCJ42tf7pr+E4GrgIWAJu38UiyJ91+fQHdfj4T+Eyb37OAPwYe1+b5YuDmIdf5KcClwMbAe4BPJMmQ0w4K8C66n5cn0G2rQ6Zbr2WsYSdgXeDzy1DXV4Bt6fbp94FPDbz3CeBvqmp94InAN1r7hMfTMixT6o2BTJoZ19L9kR/v18BmwFatx+DMmv4Gs4e0Hoa7Jnn/6Kq6sKruBN4KvKj9YV1RLwUOr6rLq+oO4CBg7zywd+7tVXVXVf0Q+CHwoGDXankxcFBV3V5VVwLvB16+ArX9V1X9qG2T4+hCFXTb91HAY6vq3qo6r6pum2wmVXVMVd1cVfdU1fuBdegC5phvVdWX2zV7Rw+s3450geZNbd/8X1WNnYb7G+BdVXVJO8X8L8AOrZfs18D6wOOBtHGuG3Kdr6qqj7VaFtP9HG06xfhfaD1HY4+/but8WVWdWlV3V9WNwOF0IXm69VqWGh4F3LQsp9ir6sj283E3XUB80liPLN122y7JI6rqF1X1/YH2ZT2epFnBQCbNjM2Bn0/Q/l7gMuCUJJcnOXCIeV2zDO9fBaxN14Oxoh7d5jc477V44B/gwU9F/pKuF228jel6scbPa/MVqG2y5R5Nd73SsUmuTfKeJGtPNpMkb2inFm9Ncgtd79rgthu/nHVbIN2SLpxMFDi2Aj4wFoTofg4CbF5V36DrZfswcH2SI5I8YlnXuap+2QYn2t5j9qyqDQceH2vrvEmSY9upv9uAYwbWear1WpYabgY2HhfeJ5VkzSSHJflJq+nK9tZYXX8BPBe4KsnpSXZq7ctzPEmzgoFMGrEkf0AXNh504XLrAXhDVT0GeD7wD0l2GXt7kllO9x//lgPD8+l6DW4C7gQeNlDXmnSndoad77V04WJw3vcA108z3Xg3tZrGz+unyzifabVekrdX1XbAHwLPozs9CuPWt10v9mbgRcAjq2pD4Fa68DSda4D5kwSOa+hOrw2GoYdW1XdajR+sqicD29OdupzwmqoRehfdtvjdqnoE8DLuX+ep1mtZfBf4P2DPIcf/S7qL/XelC8ULWnsAquqcqtqD7nTmF+h6Rac7nqRZzUAmjUiSRyR5Ht01N8dU1QUTjPO8JI9t193cBtzbHtAFnccsx6JflmS7JA+ju5D6s+2U0o/oenT+rPUSHUx3Sm7M9cCCKS6C/gzw+iRbJ1mP+685W6ZPerZajgMOTbJ+O3X3D3Q9MytVkmcm+Z0WPm+jC4KTbd/16QLmjcBaSd4GDNtbdTZwHXBYkoen+zDBzu29/wQOGrsoPd0HGvZqw3+Q5Cltf9xJF1pW+CtMltH6wB3ALUk254GBcKr1GlpV3Qq8Dfhwkj2TPCzJ2kmek+Q9E0yyPnA3Xc/aw+h+1gBI8pAkL02yQVX9mvuPm+mOJ2lWM5BJK98Xk9xO17vwFrprcib7yottga/T/UH8LvCRqjqtvfcu4OB2quuNk0w/kaOBo+hOJ60LHAC/+aP4d8DH6Xqj7qS7AHrM8e355iTf58GObPM+A7iCLjz8/TLUNejv2/Ivp+s5/HSb/8r2W8Bn6f44XwKczv3B7wPAC9N9QvCDdKc2v0IXXK+iW7/pTg8DvwmZzwceS/dhjKV018lRVZ8H3k132vQ24EK6Dy5AF/g+BvyiLfNm4H3Lv7pTGvvE7thj7AL7twO/T9cb+CXghGHWa1lV1eF0wftgutB7DfAauh6u8T5Jtz1+ClwMnDXu/ZcDV7btuR9drx5MfTxJs1q83lGSJKlf9pBJkiT1zEAmSZLUMwOZJElSzwxkkiRJPTOQSZIk9WxFv+yvVxtvvHEtWLCg7zIkSZKmdd55591UVfMmem+VDmQLFizg3HPP7bsMSZKkaSW5arL3PGUpSZLUMwOZJElSzwxkkiRJPTOQSZIk9cxAJkmS1DMDmSRJUs8MZJIkST0zkEmSJPXMQCZJktQzA5kkSVLPDGSSJEk9W6XvZTlTTrzqur5LmBX22GqzvkuQNIv5u7Lj70otD3vIJEmSejbSQJbkyiQXJPlBknNb20ZJTk3y4/b8yIHxD0pyWZJLkzx7lLVJkiTNFjPRQ/bMqtqhqha21wcCS6pqW2BJe02S7YC9ge2B3YCPJFlzBuqTJEnqVR+nLPcAFrfhxcCeA+3HVtXdVXUFcBmw48yXJ0mSNLNGHcgKOCXJeUn2bW2bVtV1AO15k9a+OXDNwLRLW5skSdJqbdSfsty5qq5NsglwapL/nWLcTNBWDxqpC3b7AsyfP3/lVClJktSjkfaQVdW17fkG4PN0pyCvT7IZQHu+oY2+FNhyYPItgGsnmOcRVbWwqhbOmzdvlOVLkiTNiJEFsiQPT7L+2DDwLOBC4CRgURttEXBiGz4J2DvJOkm2BrYFzh5VfZIkSbPFKE9Zbgp8PsnYcj5dVV9Ncg5wXJJ9gKuBvQCq6qIkxwEXA/cA+1fVvSOsT5IkaVYYWSCrqsuBJ03QfjOwyyTTHAocOqqaJEmSZiO/qV+SJKlnBjJJkqSeGcgkSZJ6ZiCTJEnqmYFMkiSpZwYySZKknhnIJEmSemYgkyRJ6pmBTJIkqWcGMkmSpJ4ZyCRJknpmIJMkSeqZgUySJKlnBjJJkqSeGcgkSZJ6ZiCTJEnqmYFMkiSpZwYySZKknhnIJEmSemYgkyRJ6pmBTJIkqWcGMkmSpJ4ZyCRJknpmIJMkSeqZgUySJKlnBjJJkqSeGcgkSZJ6ZiCTJEnqmYFMkiSpZwYySZKknhnIJEmSemYgkyRJ6pmBTJIkqWcGMkmSpJ4ZyCRJknpmIJMkSeqZgUySJKlna/VdgCQNOvGq6/ouYVbYY6vN+i5B0gyyh0ySJKlnBjJJkqSeGcgkSZJ6NvJAlmTNJP+T5OT2eqMkpyb5cXt+5MC4ByW5LMmlSZ496tokSZJmg5noIXstcMnA6wOBJVW1LbCkvSbJdsDewPbAbsBHkqw5A/VJkiT1aqSBLMkWwJ8BHx9o3gNY3IYXA3sOtB9bVXdX1RXAZcCOo6xPkiRpNhh1D9m/Af8PuG+gbdOqug6gPW/S2jcHrhkYb2lrkyRJWq2N7HvIkjwPuKGqzkvyjGEmmaCtJpjvvsC+APPnz1+REiVJmvP87r9O39/9N8oesp2B3ZNcCRwL/EmSY4Drk2wG0J5vaOMvBbYcmH4L4NrxM62qI6pqYVUtnDdv3gjLlyRJmhkjC2RVdVBVbVFVC+gu1v9GVb0MOAlY1EZbBJzYhk8C9k6yTpKtgW2Bs0dVnyRJ0mzRx62TDgOOS7IPcDWwF0BVXZTkOOBi4B5g/6q6t4f6JEmSZtSMBLKqOg04rQ3fDOwyyXiHAofORE2SJEmzhd/UL0mS1DMDmSRJUs/6uIZMmhF+lLvT90e5JUnTs4dMkiSpZwYySZKknhnIJEmSemYgkyRJ6pmBTJIkqWcGMkmSpJ4ZyCRJknpmIJMkSeqZgUySJKlnBjJJkqSeGcgkSZJ6ZiCTJEnqmYFMkiSpZwYySZKknhnIJEmSemYgkyRJ6pmBTJIkqWcGMkmSpJ4ZyCRJknpmIJMkSerZtIEsyV5J1m/DByc5Icnvj740SZKkuWGYHrK3VtXtSZ4GPBtYDPzHaMuSJEmaO4YJZPe25z8D/qOqTgQeMrqSJEmS5pZhAtlPk3wUeBHw5STrDDmdJEmShjBMsHoR8DVgt6q6BdgIeNMoi5IkSZpLpg1kVfVL4Abgaa3pHuDHoyxKkiRpLhnmU5b/BLwZOKg1rQ0cM8qiJEmS5pJhTln+ObA7cCdAVV0LrD/KoiRJkuaSYQLZr6qqgAJI8vDRliRJkjS3DBPIjmufstwwyV8DXwc+NtqyJEmS5o61phuhqt6X5E+B24DfBt5WVaeOvDJJkqQ5YtpAlmRr4MyxEJbkoUkWVNWVoy5OkiRpLhjmlOXxwH0Dr+9tbZIkSVoJhglka1XVr8ZetGFvnSRJkrSSDBPIbkyy+9iLJHsAN42uJEmSpLll2mvIgP2ATyX5EBDgGuAVI61KkiRpDhnmU5Y/AZ6aZD0gVXX76MuSJEmaO4b5lOU6wF8AC4C1kgBQVf880sokSZLmiGFOWZ4I3AqcB9w92nIkSZLmnmEC2RZVtduyzjjJusAZwDptOZ+tqn9KshHw33Q9blcCL6qqX7RpDgL2oftqjQOq6mvLulxJkqRVzTCfsvxOkt9ZjnnfDfxJVT0J2AHYLclTgQOBJVW1LbCkvSbJdsDewPbAbsBHkqy5HMuVJElapQwTyJ4GnJfk0iTnJ7kgyfnTTVSdO9rLtdujgD2Axa19MbBnG94DOLaq7q6qK4DLgB2HXxVJkqRV0zCnLJ+zvDNvPVznAY8FPlxV30uyaVVdB1BV1yXZpI2+OXDWwORLW5skSdJqbdoesqq6qqquAu6i6+Eae0yrqu6tqh2ALYAdkzxxitEz0SweNFKyb5Jzk5x74403DlOGJEnSrDZtIEuye5IfA1cAp9NdiP+VZVlIVd0CnEZ3bdj1STZr894MuKGNthTYcmCyLYBrJ5jXEVW1sKoWzps3b1nKkCRJmpWGuYbsHcBTgR9V1dbALsC3p5soybwkG7bhhwK7Av8LnAQsaqMtovtaDVr73knWSbI1sC1w9vCrIkmStGoa5hqyX1fVzUnWSLJGVX0zybuHmG4zYHG7jmwN4LiqOjnJd4HjkuwDXA3sBVBVFyU5DrgYuAfYv6ruXa61kiRJWoUME8huabdNOoPunpY30AWmKVXV+cDvTdB+M10v20TTHAocOkRNkiRJq41hTlnuAfwSeD3wVeAnwPNGWZQkSdJcMkwge1tV3VdV91TV4qr6IPDmURcmSZI0VwwTyP50grbl/m4ySZIkPdCk15Al+Vvg74Btxn0z//oM8SlLSZIkDWeqi/o/Tfd9Y++i3W+yub2qfj7SqiRJkuaQSU9ZVtWtVXUlcDDws/Zt/VsDLxv7fjFJkiStuGGuIfsccG+SxwKfoAtlnx5pVZIkSXPIMIHsvqq6B3gB8G9V9Xq6L32VJEnSSjBMIPt1kpcArwBObm1rj64kSZKkuWWYQPYqYCfg0Kq6ot1n8pjRliVJkjR3THvrpKq6GDhg4PUVwGGjLEqSJGkumTaQJdkZOATYqo0foKrqMaMtTZIkaW4Y5ubin6C7j+V5wL2jLUeSJGnuGSaQ3VpVXxl5JZIkSXPUMIHsm0neC5wA3D3WWFXfH1lVkiRJc8gwgewp7XnhQFsBf7Lyy5EkSZp7hvmU5TNnohBJkqS5atJAluQfppqwqg5f+eVIkiTNPVP1kK0/Y1VIkiTNYZMGsqp6+0wWIkmSNFcNc+skSZIkjZCBTJIkqWeTBrIkr23PO89cOZIkSXPPVD1kr2rP/z4ThUiSJM1VU33K8pIkVwLzkpw/0D52c/HfHWllkiRJc8RUn7J8SZLfAr4G7D5zJUmSJM0tU35Tf1X9DHhSkocAj2vNl1bVr0demSRJ0hwx7a2Tkjwd+CRwJd3pyi2TLKqqM0ZcmyRJ0pwwzM3FDweeVVWXAiR5HPAZ4MmjLEySJGmuGOZ7yNYeC2MAVfUjYO3RlSRJkjS3DNNDdm6STwBHt9cvBc4bXUmSJElzyzCB7G+B/YED6K4hOwP4yCiLkiRJmkumDWRVdTfddWSHj74cSZKkucd7WUqSJPXMQCZJktQzA5kkSVLPliuQJdl3ZRciSZI0Vy1vD1lWahWSJElz2HIFsqr66MouRJIkaa6aNpAl2SLJ55PcmOT6JJ9LssVMFCdJkjQXDNND9l/AScBmwObAF1ubJEmSVoJhAtm8qvqvqrqnPY4C5o24LkmSpDljmEB2U5KXJVmzPV4G3DzdREm2TPLNJJckuSjJa1v7RklOTfLj9vzIgWkOSnJZkkuTPHv5V0uSJGnVMUwgezXwIuBnwHXAC1vbdO4B3lBVTwCeCuyfZDvgQGBJVW0LLGmvae/tDWwP7AZ8JMmay7Y6kiRJq55h7mV5NbD7ss64qq6jC3BU1e1JLqG7Bm0P4BlttMXAacCbW/ux7d6ZVyS5DNgR+O6yLluSJGlVMmkgS/K2KaarqnrHsAtJsgD4PeB7wKYtrFFV1yXZpI22OXDWwGRLW9v4ee0L7Aswf/78YUuQJEmataY6ZXnnBA+Afeh6tIaSZD3gc8Drquq2qUadoK0e1FB1RFUtrKqF8+b52QJJkrTqm7SHrKrePzacZH3gtcCrgGOB90823aAka9OFsU9V1Qmt+fokm7Xesc2AG1r7UmDLgcm3AK4ddkUkSZJWVVNe1N8+EflO4Hy68Pb7VfXmqrphqunatAE+AVxSVYcPvHUSsKgNLwJOHGjfO8k6SbYGtgXOXqa1kSRJWgVNdQ3Ze4EXAEcAv1NVdyzjvHcGXg5ckOQHre0fgcOA45LsA1wN7AVQVRclOQ64mO4TmvtX1b3LuExJkqRVzlSfsnwDcDdwMPCWrsML6K71qqp6xFQzrqpvMflNyHeZZJpDgUOnmq8kSdLqZqpryJbrxuOSJElaNoYuSZKknhnIJEmSemYgkyRJ6pmBTJIkqWcGMkmSpJ4ZyCRJknpmIJMkSeqZgUySJKlnBjJJkqSeGcgkSZJ6ZiCTJEnqmYFMkiSpZwYySZKknhnIJEmSemYgkyRJ6pmBTJIkqWcGMkmSpJ4ZyCRJknpmIJMkSeqZgUySJKlnBjJJkqSeGcgkSZJ6ZiCTJEnqmYFMkiSpZwYySZKknhnIJEmSemYgkyRJ6pmBTJIkqWcGMkmSpJ4ZyCRJknpmIJMkSeqZgUySJKlnBjJJkqSeGcgkSZJ6ZiCTJEnqmYFMkiSpZwYySZKknhnIJEmSemYgkyRJ6pmBTJIkqWcjC2RJjkxyQ5ILB9o2SnJqkh+350cOvHdQksuSXJrk2aOqS5IkabYZZQ/ZUcBu49oOBJZU1bbAkvaaJNsBewPbt2k+kmTNEdYmSZI0a4wskFXVGcDPxzXvASxuw4uBPQfaj62qu6vqCuAyYMdR1SZJkjSbzPQ1ZJtW1XUA7XmT1r45cM3AeEtbmyRJ0mpvtlzUnwnaasIRk32TnJvk3BtvvHHEZUmSJI3eTAey65NsBtCeb2jtS4EtB8bbArh2ohlU1RFVtbCqFs6bN2+kxUqSJM2EmQ5kJwGL2vAi4MSB9r2TrJNka2Bb4OwZrk2SJKkXa41qxkk+AzwD2DjJUuCfgMOA45LsA1wN7AVQVRclOQ64GLgH2L+q7h1VbZIkSbPJyAJZVb1kkrd2mWT8Q4FDR1WPJEnSbDVbLuqXJEmaswxkkiRJPTOQSZIk9cxAJkmS1DMDmSRJUs8MZJIkST0zkEmSJPXMQCZJktQzA5kkSVLPDGSSJEk9M5BJkiT1zEAmSZLUMwOZJElSzwxkkiRJPTOQSZIk9cxAJkmS1DMDmSRJUs8MZJIkST0zkEmSJPXMQCZJktQzA5kkSVLPDGSSJEk9M5BJkiT1zEAmSZLUMwOZJElSzwxkkiRJPTOQSZIk9cxAJkmS1DMDmSRJUs8MZJIkST0zkEmSJPXMQCZJktQzA5kkSVLPDGSSJEk9M5BJkiT1zEAmSZLUMwOZJElSzwxkkiRJPTOQSZIk9cxAJkmS1DMDmSRJUs8MZJIkST2bdYEsyW5JLk1yWZID+65HkiRp1GZVIEuyJvBh4DnAdsBLkmzXb1WSJEmjNasCGbAjcFlVXV5VvwKOBfbouSZJkqSRmm2BbHPgmoHXS1ubJEnSamutvgsYJxO01QNGSPYF9m0v70hy6cirmh02Bm7quwgtM/fbqsn9tupy362a5sp+22qyN2ZbIFsKbDnwegvg2sERquoI4IiZLGo2SHJuVS3suw4tG/fbqsn9tupy362a3G+z75TlOcC2SbZO8hBgb+CknmuSJEkaqVnVQ1ZV9yR5DfA1YE3gyKq6qOeyJEmSRmpWBTKAqvoy8OW+65iF5txp2tWE+23V5H5bdbnvVk1zfr+lqqYfS5IkSSMz264hkyRJmnMMZCOQZEGSC1dwHo9O8tmVVZMml2TP5bkjRJJnJPnDIcbbva/bgCXZMMnf9bHsVUWS05IsbMNfbtvsAdvN43H1MexxqxW3Ir9/khyV5IUru6bZzEA2S1XVtVU1p34Ye7Qn3a26hpZkLeAZwLS/2KvqpKo6bLkqW3EbAgayIVXVc6vqFsZtN4/H1cOyHLdaKTbE3z9DM5CNzlpJFic5P8lnkzwsyZVJNgZIsjDJaW346Ul+0B7/k2T9wV62JK9MckKSryb5cZL3jC0kybOSfDfJ95Mcn2S91n5Ykovb8t/X2vZKcmGSHyY5Y8a3yAxK8rIkZ7dt+tEkaya5I8mhbf3PSrJp+095d+C9bdxt2uOrSc5LcmaSx7d5HpXk8CTfBP4b2A94fZvuj5I8P8n32j78epJN23SvTPKhgXl8MMl3klw+9h9g+6/99CTHJflR238vbetwQZJt2njzknwuyTntsXNrPyTJka235/IkB7RNcRiwTavxvTO4C3rTjp3/neD426XtmwvatlpngmnHjtEHbLdxx+OaSd7X5nN+kr9v7Q865rRikjw8yZfaMXthkhe3ffTudmycneSxbdytkixp239JkvmtfcrjtsfVmwvGH0dvar+3zk/y9rGRkryitf0wydED0//x+N+Vq7Wq8rGSH8ACujsM7NxeHwm8EbgS2Li1LQROa8NfHBh3PbpPvy4ALmxtrwQuBzYA1gWuovsC3Y2BM4CHt/HeDLwN2Ai4lPs/tLFhe74A2HywbXV8AE9o23Tt9vojwCvaPnl+a3sPcHAbPgp44cD0S4Bt2/BTgG8MjHcysGZ7fQjwxoHpHjmwzf8KeP/A/vvQwDyOp/tnaDu6e7dC91/7LcBmwDrAT4G3t/deC/xbG/408LQ2PB+4ZKCW77RpNwZuBtYe/DmaK49Jjr+D6W7L9rjW9kngdW34NGBhG76ybb8HbDceeDz+LfA5YK32eqPJjjkfK7wv/wL42MDrDdo+ekt7/Qrg5Db8RWBRG3418IU2POVx62Ok+2/wuHkW3Scp037/nQz8MbB9O3bG/jZuNLDfHvS7cnV+zLqvvViNXFNV327DxwAHTDHut4HDk3wKOKGqliYPuovUkqq6FSDJxXS3X9iQ7gf12238hwDfBW4D/g/4eJIv0f3gjy3nqCTHASes2OrNarsATwbOadvlocANwK+4f1ucB/zp+AnT9TD+IXD8wD4Y7Ek5vqrunWS5WwD/nWQzun1xxSTjfaGq7gMuHutFa86pqutaHT8BTmntFwDPbMO7AtsN1PaIJOu34S9V1d3A3UluAAbnPdeMP/7eClxRVT9qbYuB/YF/W4557wr8Z1XdA1BVP093KmyiY04r5gLgfUneTRe8zmw/+59p738G+Nc2vBPwgjZ8NN0/XWOmOm41M57VHv/TXq8HbAs8CfhsVd0E3fE0MM1kvytXSway0Rn/fSIF3MP9p4nX/c0bVYe1X+LPBc5KsivdL/dBdw8M30u37wKcWlUvGb/wJDvSBZO9gdcAf1JV+yV5CvBnwA+S7FBVNy/vCs5iARZX1UEPaEzeWO1fL+7fhuOtAdxSVTtMMu87p1juvwOHV9VJSZ5B95/4RAb3ZSZpv2/g9X0Dta4B7FRVdw3OsP2RmuhnZK4a5ff5ZPz8q/tS6wcdcyOsYU6oqh8leTLd78Z3JRn7J2Vw+0+2rwfbpzpuNTMCvKuqPvqAxu7yisn24WS/K1dLXkM2OvOT7NSGXwJ8i66r/cmt7S/GRkyyTVVdUFXvBs4FHj/kMs4Cdh64huJhSR7Xenk2qO5Ldl8H7DCwnO9V1dvobuK65cSzXeUtAV6YZBOAJBslmfSGrsDtwPoAVXUbcEWSvdq0SfKk6aZrNqA71QiwaAXqn8opdH/sAUiywzTjj69xrhh//H0dWDB2rAAvB06fYvqpttspwH6tV2zs52vCY04rJsmjgV9W1THA+4Dfb2+9eOD5u234O3RhGOCldL9zJzJXj4k+DG7rrwGvzv3XOW/efkcvAV6U5FGtfaNeKp0FDGSjcwmwKMn5dNeX/AfwduADSc6k68EY87p2weoPgbuArwyzgKq6ke76pM+05ZxFF+bWB05ubacDr2+TvLddiHwh3bVnP1zBdZyVqupiumuGTmnb4FS6a7MmcyzwpnbB9zZ0v8z3afvjImCPSab7IvDnAxcHH0J3qvNMusA7CgcAC9sFsBfTXaA8qdYD+u328zUnLupvxh9//wq8im7/XEDX6/ifk008zXb7OHA1cH77GflLJj/mtGJ+Bzg7yQ+AtwDvbO3rJPke3fWVY9v6AOBVbR+8vL03kfHHrUZk8Diiu0Tk08B32zH4WWD96m6PeChwejueDu+t4J75Tf2SVitJFtBdb/TEvmvRypfkSroPYYzqnx6pF/aQSZIk9cweMkmSpJ7ZQyZJktQzA5kkSVLPDGSSJEk9M5BJ6k2SR+X++7j+LMlPB14/ZMh5/OMU762X7l6mP0lyUZIz2pcjL9f8VqYkuyc5cCaWJWn286J+SbNCkkOAO6pqmW7MneSOqlpvkveOpbuF1Vuq6r4kjwGeUFVfWp75rSxJ1hq79ZIkgT1kkmaZJE9OcnqS85J8LclmSTZIcmmS327jfCbJXyc5DHho61H71Lj5bEN3c/iD2/3wqKrLx8JYki+0ZVyUZN/W9qD5JXlZkrNb20eTrNna90nyoySnJflYkg+19q2SLGlf3rskyfzWflSSw5N8E3h3klcOTDMvyeeSnNMeO7f2pw/0GP5P7r9vqaTVjIFM0mwSunuCvrCqngwcCRxaVbfS3TLqqCR7A4+sqo9V1YHAXVW1Q1W9dNy8tgd+MMVNpV/dlrEQOCDJo8bPL8kT6G7Ps3O7v+m9wEvbLX3eCjyV7hvIB2939iHgk1X1u8CngA8OvPc4YNeqesO4Wj4A/GtV/QHdbdU+3trfCOzflv1HdHfykLQamss3H5Y0+6wDPBE4Nd0N09cErgOoqlPT3WP0w8Bk9xddFgck+fM2vCWwLXDzuHF2obv/7DmtnocCNwA7AqdX1c8BkhxPF7YAdgJe0IaPBt4zML/jJwmIuwLbtWUAPKL1hn0bOLz11p1QVUuXZ0UlzX4GMkmzSYCLqmqnB72RrAE8ga6XaCNgunByEfCkJGuMnbIcmNcz6ELQTlX1yySnAetOUs/iqjpo3PR/PsG4kxm8UPfOScZZo9UyvgfssCRfAp4LnJVk16r632VYtqRVhKcsJc0mdwPzkuwEkGTtJNu3915Pd9PwlwBHJlm7tf96YPg3quonwLnA29O6npJsm2QPYAPgFy2MPZ7u1CMTzG8J8MIkm7TpN0qyFXA28PQkj0yyFt1pxjHfAfZuwy8FvjXEep9Cd0qWtpwd2vM2VXVBVb27rcvjJ55c0qrOQCZpNrkPeCHdRe8/BH4A/GGSxwF/Bbyhqs4EzgAObtMcAZw//qL+5q+A3wIuS3IB8DHgWuCrwFpJzgfeAZw1MM1v5ldVF7flnNLGPRXYrKp+CvwL8D3g68DFwK1t+gOAV7XxXw68doj1PgBY2D4IcDGwX2t/XZIL27a4C/jKEPOStAryay8kaTkkWa+q7mg9ZJ8Hjqyqz/ddl6RVkz1kkrR8DknyA+BCuu86+0Kv1UhapdlDJkmS1DN7yCRJknpmIJMkSeqZgUySJKlnBjJJkqSeGcgkSZJ6ZiCTJEnq2f8H+CBZ0krSm74AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "categories = list(category_dict.keys())\n",
    "instances = list(category_dict.values())\n",
    "  \n",
    "fig = plt.figure(figsize = (10, 5))\n",
    " \n",
    "# Creating the bar plot\n",
    "plt.bar(categories, instances, color ='powderblue', width = 0.6)\n",
    " \n",
    "plt.xlabel(\"Text Categories\")\n",
    "plt.ylabel(\"No. of Instances\")\n",
    "plt.title(\"Distribution of Instances in Each Class\")\n",
    "plt.savefig(\"../output/BBC-distribution.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18330d3d",
   "metadata": {},
   "source": [
    "#### 3. Load the corpus using load files and make sure you set the encoding to latin1. This will read the file structure and assign the category name to each file from their parent directory name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34b11d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files\n",
    "\n",
    "# Reads the file structure and assign the category name to each file from their parent directory name\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_files.html\n",
    "res = load_files(\"../data/BBC\", encoding=\"latin-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd45fc7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The names of target classes:  ['business', 'entertainment', 'politics', 'sport', 'tech']\n",
      "\n",
      "Some examples below: \n",
      "Tate & Lyle boss bags top awar...business\n",
      "Halo 2 sells five million copi...tech\n",
      "MSPs hear renewed climate warn...politics\n",
      "Pavey focuses on indoor succes...sport\n",
      "Tories reject rethink on axed ...politics\n",
      "Lib Dems predict 'best ever po...politics\n",
      "Howard attacks 'pay later' Bud...politics\n",
      "Games win for Blu-ray DVD form...tech\n",
      "Labour pig poster 'anti-Semiti...politics\n",
      "Costin aims for comeback in 20...sport\n"
     ]
    }
   ],
   "source": [
    "# The raw text data to learn (list of str)\n",
    "X = res.data\n",
    "\n",
    "# The target labels (e.g. business/entertainment/politics/sport/tech) but as an integer index! (e.g. 0/1/2/3/4)\n",
    "y = res.target\n",
    "\n",
    "target_names = res.target_names\n",
    "print(\"The names of target classes: \", target_names)\n",
    "\n",
    "print(\"\\nSome examples below: \")\n",
    "for i in range(0,10):\n",
    "    text = res.data[i]\n",
    "    target = res.target[i]\n",
    "    category = target_names[target]\n",
    "    print('{text}...{category}'.format(text=text[0:30], category=category))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394c3b86",
   "metadata": {},
   "source": [
    "#### 5. Split the dataset into 80% for training and 20% for testing. For this, you must use train test split with the parameter random state set to None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a05b46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05913df",
   "metadata": {},
   "source": [
    "#### 4. Pre-process the dataset to have the features ready to be used by a multinomial Naive Bayes classifier. This means that the frequency of each word in each class must be computed and stored in a term-document matrix. For this, you can use feature extraction.text.CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ee6f80d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train_counts:  (1668, 26123)\n",
      "y_train length:  1668\n",
      "\n",
      "Vocabulary (A Mapping of Terms to Feature Indices)\n",
      "dominici: 7802\n",
      "backs: 2898\n",
      "lacklustre: 13659\n",
      "france: 9981\n",
      "wing: 25670\n",
      "christophe: 5143\n",
      "says: 20605\n",
      "can: 4515\n",
      "claim: 5225\n",
      "another: 2227\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Convert a collection of text documents to a matrix of token counts. (\"Tokenization\")\n",
    "count_vect = CountVectorizer()\n",
    "\n",
    "# Xarray of shape (n_samples, n_features)\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer.fit_transform\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "\n",
    "print(\"Shape of X_train_counts: \", X_train_counts.shape)\n",
    "print(\"y_train length: \", len(y_train))\n",
    "\n",
    "print(\"\\nVocabulary (A Mapping of Terms to Feature Indices)\")\n",
    "vocabulary = count_vect.vocabulary_\n",
    "names = list(vocabulary.keys())\n",
    "index = list(vocabulary.values())\n",
    "for i in range(0,10):\n",
    "    print(names[i] + \":\",index[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba4bd64",
   "metadata": {},
   "source": [
    "#### 6. Train a multinomial Naive Bayes Classifier (naive bayes.MultinomialNB) on the training set using the default parameters and evaluate it on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32e5af8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "multinomialNB = MultinomialNB()\n",
    "clf = multinomialNB.fit(X_train_counts, y_train)\n",
    "\n",
    "# print(sum(multinomialNB.feature_count_[1,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f56a29dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's test with a few examples to see if the model makes sense...\n",
      "\n",
      "'MSFT stock hit $300' => business\n",
      "'Intel core processor with 16GB RAM' => tech\n",
      "'GO HABS GO!' => sport\n"
     ]
    }
   ],
   "source": [
    "print(\"Let's test with a few examples to see if the model makes sense...\\n\")\n",
    "\n",
    "docs_new = ['MSFT stock hit $300', 'Intel core processor with 16GB RAM', \"GO HABS GO!\"]\n",
    "X_new_counts = count_vect.transform(docs_new)\n",
    "\n",
    "predicted = clf.predict(X_new_counts)\n",
    "\n",
    "for doc, category in zip(docs_new, predicted):\n",
    "    print('%r => %s' % (doc, target_names[category]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2998cf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's actually test the model using X_text, y_test\n",
    "\n",
    "X_test_counts = count_vect.transform(X_test)\n",
    "\n",
    "y_predicted = clf.predict(X_test_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92c54e8",
   "metadata": {},
   "source": [
    "#### 7. In a file called bbc-performance.txt, save the following information: (to make it easier for the TAs, make sure that your output for each sub-question below is clearly marked in your output file, using the headings (a), (b) . . .)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdb48e4",
   "metadata": {},
   "source": [
    "##### (a) a clear separator (a sequence of hyphens or stars) and string clearly describing the model (e.g. “MultinomialNB default values, try 1”)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3900ab79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to file...\n"
     ]
    }
   ],
   "source": [
    "# Create a file that can be appended to\n",
    "file_performance = open(\"../output/bbc-performance.txt\", \"a\")\n",
    "\n",
    "# Clear contents\n",
    "file_performance.truncate(0)\n",
    "\n",
    "def write_model_name_to_file(model_name):\n",
    "    file_performance.write(\"\\n(a) **** {model_name} ****\\n\\n\".format(model_name=model_name))\n",
    "\n",
    "write_model_name_to_file(\"MultinomialNB default values, try 1\")\n",
    "print(\"Writing to file...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5aeef6",
   "metadata": {},
   "source": [
    "##### (b) the confusion matrix (you can use confusion matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "785f8b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[117   0   5   0   2]\n",
      " [  0 102   4   0   1]\n",
      " [  2   0  96   0   0]\n",
      " [  1   0   0 124   0]\n",
      " [  0   0   2   0 101]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cf_matrix = confusion_matrix(y_test, y_predicted)\n",
    "\n",
    "def print_confusion_matrix(cf_matrix):\n",
    "    print('Confusion Matrix:\\n {cf_matrix}'.format(cf_matrix=cf_matrix))\n",
    "    \n",
    "def write_confusion_matrix_to_file(cf_matrix):\n",
    "    file_performance.write(\"(b) Confusion Matrix\" + \"\\n\\n\")\n",
    "    np.savetxt(file_performance, X=cf_matrix.astype(int), fmt ='%i\\t')\n",
    "    file_performance.write(\"\\n\")\n",
    "    \n",
    "print_confusion_matrix(cf_matrix)\n",
    "write_confusion_matrix_to_file(cf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa20e08",
   "metadata": {},
   "source": [
    "##### (c) the precision, recall, and F1-measure for each class (you can use classification report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "648f6a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.97      0.94      0.96       124\n",
      "entertainment       1.00      0.95      0.98       107\n",
      "     politics       0.90      0.98      0.94        98\n",
      "        sport       1.00      0.99      1.00       125\n",
      "         tech       0.97      0.98      0.98       103\n",
      "\n",
      "     accuracy                           0.97       557\n",
      "    macro avg       0.97      0.97      0.97       557\n",
      " weighted avg       0.97      0.97      0.97       557\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def print_classification_report(y_test, y_predicted, target_names):\n",
    "    report = metrics.classification_report(y_test, y_predicted, target_names=target_names)\n",
    "    print(report)\n",
    "    \n",
    "def write_classification_report_to_file(y_test, y_predicted, target_names):\n",
    "    report = metrics.classification_report(y_test, y_predicted, target_names=target_names)\n",
    "    file_performance.write(\"(c) Classification Report\\n\\n\" + report)\n",
    "    file_performance.write(\"\\n\")\n",
    "    \n",
    "print_classification_report(y_test, y_predicted, target_names)\n",
    "write_classification_report_to_file(y_test, y_predicted, target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2177baca",
   "metadata": {},
   "source": [
    "##### (d) the accuracy, macro-average F1 and weighted-average F1 of the model (you can use accuracy score and f1 score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8a92ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:  0.9694793536804309\n",
      "Macro-Average F1:  0.9687015321382393\n",
      "Weighted-Average F1:  0.9697552231430182\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def print_scores(y_test,y_predicted):\n",
    "    print(\"Accuracy Score: \", accuracy_score(y_test, y_predicted))\n",
    "    print(\"Macro-Average F1: \", f1_score(y_test, y_predicted, average=\"macro\"))\n",
    "    print(\"Weighted-Average F1: \", f1_score(y_test, y_predicted, average=\"weighted\"))\n",
    "    \n",
    "def write_scores_to_file(y_test,y_predicted):\n",
    "    file_performance.write(\"(d) Accuracy, Macro-Average F1 and Weighted-Average F1\\n\\n\")\n",
    "    file_performance.write(\"Accuracy Score: \" + str(accuracy_score(y_test, y_predicted)) + \"\\n\")\n",
    "    file_performance.write(\"Macro-Average F1: \" + str(f1_score(y_test, y_predicted, average=\"macro\")) + \"\\n\")\n",
    "    file_performance.write(\"Weighted-Average F1: \" + str(f1_score(y_test, y_predicted, average=\"weighted\")) + \"\\n\")\n",
    "    file_performance.write(\"\\n\")\n",
    "\n",
    "print_scores(y_test, y_predicted)\n",
    "write_scores_to_file(y_test,y_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b4a9a7",
   "metadata": {},
   "source": [
    "##### (e) the prior probability of each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "551ca2f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'business': 510, 'entertainment': 386, 'politics': 417, 'sport': 511, 'tech': 401}\n",
      "Total Number of Texts: 2225\n"
     ]
    }
   ],
   "source": [
    "# Recall at the beginning we counted the size of each class\n",
    "print(category_dict)\n",
    "print('Total Number of Texts: {num_texts}'.format(num_texts=num_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86e524d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of samples encountered for each class: [386. 279. 319. 386. 298.]\n",
      "The total number of samples: 1668.0\n",
      "The prior probabilities of each class: {'business': 0.2314, 'entertainment': 0.1673, 'politics': 0.1912, 'sport': 0.2314, 'tech': 0.1787}\n"
     ]
    }
   ],
   "source": [
    "# Returns a dictionary that holds the prior probabilities of each class\n",
    "def compute_prior_probabilities(multinomialNB):\n",
    "    class_count = multinomialNB.class_count_\n",
    "    total = sum(class_count)\n",
    "    prior_dict = dict()\n",
    "    \n",
    "    for i in range(len(class_count)):\n",
    "        target = target_names[i]\n",
    "        probability = class_count[i] / total\n",
    "        prior_dict[target] = round(probability, 4)\n",
    "        \n",
    "    return prior_dict\n",
    "\n",
    "def print_class_count(multionomialNB):\n",
    "    class_count = multinomialNB.class_count_\n",
    "    total = sum(class_count)\n",
    "    print(\"The number of samples encountered for each class: {class_count}\".format(class_count=class_count))\n",
    "    print(\"The total number of samples: {total}\".format(total=total))\n",
    "    \n",
    "def print_prior_probabilities(multinomialNB):\n",
    "    prior_dict = compute_prior_probabilities(multinomialNB)\n",
    "    print(\"The prior probabilities of each class: {prior_dict}\".format(prior_dict=prior_dict))\n",
    "    \n",
    "def write_prior_probabilities_to_file(multinomialNB):\n",
    "    file_performance.write(\"(e) Prior Probability of Each Class F1\\n\\n\")\n",
    "    prior_dict = compute_prior_probabilities(multinomialNB)\n",
    "    for key in prior_dict:\n",
    "        category = key\n",
    "        probability = prior_dict[key]\n",
    "        file_performance.write(\"{category}: {probability:.4f}\".format(category=category, probability=probability))\n",
    "        file_performance.write(\"\\n\")\n",
    "    file_performance.write(\"\\n\")\n",
    "    \n",
    "print_class_count(multinomialNB)\n",
    "print_prior_probabilities(multinomialNB)\n",
    "write_prior_probabilities_to_file(multinomialNB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf35119",
   "metadata": {},
   "source": [
    "##### (f) the size of the vocablary (i.e. the number of different words). For example, if the word potato appears 3 times, you only count it once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2def2d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the vocabulary: 26123\n",
      "\n",
      "Note that this can also be deduced by the number of columns (features) in the document-term matrix:  26123\n",
      "Or this can be computed from the feature counts of the MultionomialNB:  26123\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(vocabulary.keys())\n",
    "\n",
    "def print_vocabulary_size(multinomialNB):\n",
    "    print(\"The size of the vocabulary: {vocabulary_size}\\n\".format(vocabulary_size = vocabulary_size))\n",
    "    print(\"Note that this can also be deduced by the number of columns (features) in the document-term matrix: \", X_train_counts.shape[1])\n",
    "    print(\"Or this can be computed from the feature counts of the MultionomialNB: \", multinomialNB.feature_count_.shape[1])\n",
    "\n",
    "def write_vocabulary_size_to_file(multinomialNB):\n",
    "    file_performance.write(\"(f) The Size of the Vocablary\\n\")\n",
    "    # The number of columns in the term document matrix\n",
    "    vocabulary_size = multinomialNB.feature_count_.shape[1]\n",
    "    file_performance.write(\"The size of the vocabulary: {vocabulary_size}\\n\".format(vocabulary_size = vocabulary_size))\n",
    "    file_performance.write(\"\\n\")\n",
    "\n",
    "print_vocabulary_size(multinomialNB)\n",
    "write_vocabulary_size_to_file(multinomialNB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5ec8ed",
   "metadata": {},
   "source": [
    "##### (g) the number of word-tokens in each class (i.e. the number of words in total). For example, if the word potato appears 3 times, you count it 3 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c6103845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of word tokens by class:  {'business': 123791.0, 'entertainment': 89372.0, 'politics': 142682.0, 'sport': 120788.0, 'tech': 144985.0}\n"
     ]
    }
   ],
   "source": [
    "# generates a dictionary containing the number of word-tokens in each class\n",
    "def compute_word_token_dict(multinomialNB):\n",
    "    word_token_dict = dict()\n",
    "    class_count = multinomialNB.class_count_\n",
    "    feature_count = multinomialNB.feature_count_\n",
    "    \n",
    "    for i in range(len(class_count)):\n",
    "        target = target_names[i]\n",
    "        # Take the sum of all the words for that class\n",
    "        word_token_dict[target] = sum(feature_count[i, :])\n",
    "        \n",
    "    return word_token_dict\n",
    "    \n",
    "def print_word_tokens_by_class(multinomialNB):\n",
    "    word_token_dict = compute_word_token_dict(multinomialNB)\n",
    "    print(\"The number of word tokens by class: \", word_token_dict)\n",
    "    \n",
    "def write_word_tokens_by_class_to_file(multinomialNB):\n",
    "    file_performance.write(\"(g) The number of word-tokens in each class (i.e. the number of words in total)\\n\")\n",
    "    word_token_dict = compute_word_token_dict(multinomialNB)\n",
    "    file_performance.write(\"The number of word tokens by class: \" +  str(word_token_dict))\n",
    "    file_performance.write(\"\\n\\n\")\n",
    "\n",
    "print_word_tokens_by_class(multinomialNB)\n",
    "write_word_tokens_by_class_to_file(multinomialNB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c92c7e2",
   "metadata": {},
   "source": [
    "##### (h) the number of word-tokens in the entire corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0ed95f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total word-tokens in the corpus:  621618.0\n"
     ]
    }
   ],
   "source": [
    "def compute_total_word_\n",
    "\n",
    "def print_word_tokens_total(multinomialNB):\n",
    "    feature_count_ = multinomialNB.feature_count\n",
    "    total = feature_count.sum()\n",
    "    print(\"Total word-tokens in the corpus: \", total)\n",
    "\n",
    "def write_word_tokens_total_to_file(multinomialNB):\n",
    "    file_performance.write(\"(h) The number of word-tokens in the entire corpus\\n\")\n",
    "    feature_count_ = multinomialNB.feature_count\n",
    "    total = feature_count.sum()\n",
    "    file_performance.write(\"Total word-tokens in the corpus: \" + str(total))\n",
    "    file_performance.write(\"\\n\\n\")\n",
    "    \n",
    "print_word_tokens_total(multinomialNB)\n",
    "write_word_tokens_total_to_file(multinomialNB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf2cb29",
   "metadata": {},
   "source": [
    "##### (i) the number and percentage of words with a frequency of zero in each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ca10b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall that the number of features is: 26123\n",
      "The number and percentage of words with a frequency of ZERO in each class is outlined below\n",
      "\n",
      "Count for business: 15752\n",
      "Percentage for business: 60.30%\n",
      "\n",
      "\n",
      "Count for entertainment: 16318\n",
      "Percentage for entertainment: 62.47%\n",
      "\n",
      "\n",
      "Count for politics: 15918\n",
      "Percentage for politics: 60.93%\n",
      "\n",
      "\n",
      "Count for sport: 16765\n",
      "Percentage for sport: 64.18%\n",
      "\n",
      "\n",
      "Count for tech: 15581\n",
      "Percentage for tech: 59.64%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_count_and_percentage(x, y, class_name):\n",
    "    print(\"Count for {class_name}: {count}\".format(class_name=class_name, count=x))\n",
    "    print(\"Percentage for {class_name}: {percentage:.2%}\".format(class_name=class_name, percentage=x/y))\n",
    "    print(\"\\n\")\n",
    "\n",
    "def write_count_and_percentage_to_file(x, y, class_name):\n",
    "    file_performance.write(\"Count for {class_name}: {count}\".format(class_name=class_name, count=x))\n",
    "    file_performance.write(\"\\n\")\n",
    "    file_performance.write(\"Percentage for {class_name}: {percentage:.2%}\".format(class_name=class_name, percentage=x/y))\n",
    "    file_performance.write(\"\\n\")\n",
    "    \n",
    "# returns a dictionary containing the number of words with a frequency zero in each class\n",
    "def compute_frequency_zero_words(multinomialNB):\n",
    "    frequency_zero_dict = dict()\n",
    "    feature_count = multinomialNB.feature_count_\n",
    "    num_features = feature_count.shape[1]\n",
    "    \n",
    "    for i in range(len(target_names)):\n",
    "        target = target_names[i]\n",
    "        count_zero = num_features - np.count_nonzero(feature_count[i,:])\n",
    "        frequency_zero_dict[target] = count_zero\n",
    "        \n",
    "    return frequency_zero_dict\n",
    "    \n",
    "def print_words_with_frequency_zero(multinomialNB):\n",
    "    feature_count = multinomialNB.feature_count_\n",
    "    num_features = feature_count.shape[1]\n",
    "    frequency_zero_dict = compute_frequency_zero_words(multinomialNB)\n",
    "    print(\"Recall that the number of features is: {num_features}\".format(num_features = num_features))\n",
    "    print(\"The number and percentage of words with a frequency of ZERO in each class is outlined below\\n\")\n",
    "    \n",
    "    for key in frequency_zero_dict:\n",
    "        print_count_and_percentage(frequency_zero_dict[key], num_features, key)\n",
    "    \n",
    "def write_words_with_frequency_zero_to_file(multinomialNB):\n",
    "    file_performance.write(\"(i) the number and percentage of words with a frequency of zero in each class\\n\")\n",
    "    feature_count = multinomialNB.feature_count_\n",
    "    num_features = feature_count.shape[1]\n",
    "    frequency_zero_dict = compute_frequency_zero_words(multinomialNB)\n",
    "    \n",
    "    for key in frequency_zero_dict:\n",
    "        write_count_and_percentage_to_file(frequency_zero_dict[key], num_features, key)\n",
    "    \n",
    "print_words_with_frequency_zero(multinomialNB)\n",
    "write_words_with_frequency_zero_to_file(multinomialNB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91956559",
   "metadata": {},
   "source": [
    "##### (j) the number and percentage of words with a frequency of one in the entire corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "41cd29b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'toarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-33461e7b8840>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mfile_performance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[0mnum_words_with_frequency_one\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_num_words_with_frequency_one\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[0mprint_num_words_with_frequency_one\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_words_with_frequency_one\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocabulary_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-33461e7b8840>\u001b[0m in \u001b[0;36mget_num_words_with_frequency_one\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m# Iterate through the rows of the term-document matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mX_array\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'toarray'"
     ]
    }
   ],
   "source": [
    "def get_num_words_with_frequency_one(multinomialNB):\n",
    "    feature_count = multinomialNB.feature_count_\n",
    "    frequency_zero_dict = compute_frequency_zero_words(multinomialNB)\n",
    "\n",
    "def print_num_words_with_frequency_one(total, vocabulary_size):\n",
    "    print(\"The number of words with a frequency of one in the entire corpus: \", total)\n",
    "    print(\"The percentage of words with a frequency of one in the entire corpus: {percentage:.2%}\".format(percentage=(total/vocabulary_size)))\n",
    "    \n",
    "def write_num_words_with_frequency_one_to_file(total, vocabulary_size):\n",
    "    file_performance.write(\"(j) the number and percentage of words with a frequency of one in the entire corpus\\n\")\n",
    "    file_performance.write(\"The number of words: \" + str(total))\n",
    "    file_performance.write(\"\\n\")\n",
    "    file_performance.write(\"The percentage of words: {percentage:.2%}\".format(percentage=(total/vocabulary_size)))\n",
    "    file_performance.write(\"\\n\\n\")\n",
    "    \n",
    "# num_words_with_frequency_one = get_num_words_with_frequency_one()\n",
    "\n",
    "# print_num_words_with_frequency_one(num_words_with_frequency_one, vocabulary_size)\n",
    "# write_num_words_with_frequency_one_to_file(num_words_with_frequency_one, vocabulary_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efd3ff2",
   "metadata": {},
   "source": [
    "##### (k) your 2 favorite words (that are present in the vocabulary) and their log-prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221575d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_index_1 = vocabulary[\"executive\"]\n",
    "feature_index_2 = vocabulary[\"the\"]\n",
    "\n",
    "print(\"Index of word 'executive': {index}\".format(index = feature_index_1))\n",
    "print(\"Index of word 'the': {index}\".format(index = feature_index_2))\n",
    "\n",
    "p1 = multinomialNB.feature_log_prob_\n",
    "\n",
    "log_prob_1 = sum(p1[:, feature_index_1])\n",
    "log_prob_2 = sum(p1[:, feature_index_2])\n",
    "\n",
    "def print_log_prob():\n",
    "    print(\"\\n\")\n",
    "    print(\"log_prob of word 'executive': \", str(log_prob_1))\n",
    "    print(\"log_prob of word 'the': \", str(log_prob_2))\n",
    "    \n",
    "def write_log_prob_to_file():\n",
    "    file_performance.write(\"(k) your 2 favorite words (that are present in the vocabulary) and their log-prob\\n\")\n",
    "    file_performance.write(\"log_prob of word 'executive': \" + str(log_prob_1))\n",
    "    file_performance.write(\"\\n\")\n",
    "    file_performance.write(\"log_prob of word 'the': \" + str(log_prob_2))\n",
    "    file_performance.write(\"\\n\")\n",
    "\n",
    "print_log_prob()\n",
    "write_log_prob_to_file()\n",
    "\n",
    "# Makes sense that the log_prob of \"the\" is higher than the log_prob of \"executive\"!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87dd09b6",
   "metadata": {},
   "source": [
    "#### 8. Redo steps 6 and 7 without changing anything (do not redo step 5, the dataset split). Change the model name to something like “MultinomialNB default values, try 2” and append the results to the file bbc-performance.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf1907c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_separator_to_file():\n",
    "    file_performance.write(\"\\n---------------------------------------------------------------------\\n\")\n",
    "\n",
    "def write_model_to_file(model_name, cf_matrix, y_test, y_predicted, target_names):\n",
    "    write_separator_to_file()\n",
    "    write_model_name_to_file(model_name)\n",
    "    write_confusion_matrix_to_file(cf_matrix)\n",
    "    write_classification_report_to_file(y_test, y_predicted, target_names)\n",
    "    write_scores_to_file(y_test, y_predicted2)\n",
    "    write_prior_probabilities_to_file(prior_dict)\n",
    "    write_vocabulary_size_to_file(vocabulary_size)\n",
    "    write_word_tokens_by_class_to_file(word_tokens_by_class)\n",
    "    write_word_tokens_total_to_file()\n",
    "    write_words_with_frequency_zero_to_file()\n",
    "    write_num_words_with_frequency_one_to_file(num_words_with_frequency_one, vocabulary_size)\n",
    "    write_log_prob_to_file()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3d9515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multinomial NB, take #2\n",
    "\n",
    "clf2 = multinomialNB.fit(X_train, y_train)\n",
    "y_predicted2 = clf2.predict(X_test)\n",
    "\n",
    "cf_matrix2 = confusion_matrix(y_test, y_predicted2)\n",
    "\n",
    "print(cf_matrix2)\n",
    "print(\"\\n\")\n",
    "print_classification_report(y_test, y_predicted2, target_names)\n",
    "print_scores(y_test, y_predicted2)\n",
    "\n",
    "write_model_to_file(\"MultinomialNB default values, try 2\", cf_matrix2, y_test, y_predicted2, target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0245b205",
   "metadata": {},
   "source": [
    "#### 9. Redo steps 6 and 7 again, but this time, change the smoothing value to 0.0001. Append the results at the end of bbc-performance.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25db5fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "multinomialNB3 = MultinomialNB(alpha = 0.0001)\n",
    "\n",
    "clf3 = multinomialNB3.fit(X_train, y_train)\n",
    "y_predicted3 = clf3.predict(X_test)\n",
    "\n",
    "cf_matrix3 = confusion_matrix(y_test, y_predicted3)\n",
    "\n",
    "print(cf_matrix3)\n",
    "print(\"\\n\")\n",
    "print_classification_report(y_test, y_predicted3, target_names)\n",
    "print_scores(y_test, y_predicted3)\n",
    "\n",
    "write_model_to_file(\"MultinomialNB (smoothing = 0.0001)\", cf_matrix3, y_test, y_predicted3, target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7342ae9d",
   "metadata": {},
   "source": [
    "#### 10. Redo steps 6 and 7, but this time, change the smoothing value to 0.9. Append the results at the end of bbc-performance.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608fc129",
   "metadata": {},
   "outputs": [],
   "source": [
    "multinomialNB4 = MultinomialNB(alpha = 0.9)\n",
    "\n",
    "clf4 = multinomialNB4.fit(X_train, y_train)\n",
    "y_predicted4 = clf4.predict(X_test)\n",
    "\n",
    "cf_matrix4 = confusion_matrix(y_test, y_predicted4)\n",
    "\n",
    "print(cf_matrix4)\n",
    "print(\"\\n\")\n",
    "print_classification_report(y_test, y_predicted4, target_names)\n",
    "print_scores(y_test, y_predicted4)\n",
    "\n",
    "write_model_to_file(\"MultinomialNB (smoothing = 0.9)\", cf_matrix4, y_test, y_predicted4, target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92bf18b",
   "metadata": {},
   "source": [
    "#### 11. In a separate plain text file called bbc-discussion.txt, explain in 1 to 2 paragraphs:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a66e8c9",
   "metadata": {},
   "source": [
    "##### (a) what metric is best suited to this dataset/task and why (see step (2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214d61f3",
   "metadata": {},
   "source": [
    "##### (b) why the performance of steps (8-10) are the same or are different than those of step (7) above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9152d124",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_performance.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
