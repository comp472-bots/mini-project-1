Question 11
 
(a) 

First and foremost, it is necessary to discuss the class balance of business/entertainment/politics/sport/tech
As illustrated in the first section, the classes are MOSTLY balanced (with some minor discrepancies)

Thus, it can be said that accuracy along with F1 measure (macro and weighted) are suitable measures for this dataset.

This is highlighted by the fact that the accuracy score, macro-average F1, and weighted-average F1 all
have a similar result of 0.97-0.98!

If it was the case that there is a major class imbalance, weighted-average F1 would be much more suitable in
order to account for the classes that are represented less.

-----------------------------------------------------------------------------------------------------------------------

(b)

There are many interesting observations that can be drawn from steps 8-10. First and foremost, it is important
to note that step 8 (MultinomialNB with default values) yielded the same results as step 6. 
--> This is expected behavior given that we kept the same values and did not redo the test split!

Moreover, the performance of the MultinomialNB models were very similar for steps (8-10). All the models achieved
a similar performance in terms of accuracy, macro-average F1, and weighted-average F1. 
(There is a very small differences in the confusion matrices).

Lastly, a small change that can be observed is in the feature log probabilities of the words (Question #7 (k)) 
when changing the smoothing values.

Note that this is expected, since smoothing was applied!!!

